{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ufi3gmUD_rbh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Only need to run the first time.\n",
        "# Works with latest triton. Sorry, this takes a minute to install.\n",
        "!pip install jaxtyping\n",
        "!pip install git+https://github.com/Deep-Learning-Profiling-Tools/triton-viz\n",
        "!wget \"https://dl.cloudsmith.io/public/test-wha/triton-puzzles/raw/files/triton-3.0.0-cp310-cp310-linux_x86_64.whl\"\n",
        "!mv triton-3.0.0*.whl triton-3.0.0-cp310-cp310-linux_x86_64.whl\n",
        "!pip install triton-3.0.0-cp310-cp310-linux_x86_64.whl\n",
        "!export LC_ALL=\"en_US.UTF-8\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
        "!export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
        "!ldconfig /usr/lib64-nvidia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B_GXZAGsAGc5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import triton\n",
        "from torch import Tensor\n",
        "import triton.language as tl\n",
        "import jaxtyping\n",
        "from jaxtyping import Float32, Int32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OFjkmeS0ALxa"
      },
      "outputs": [],
      "source": [
        "# @title Setup\n",
        "\n",
        "import triton_viz\n",
        "import inspect\n",
        "from triton_viz.interpreter import record_builder\n",
        "\n",
        "def test(puzzle, puzzle_spec, nelem={}, B={\"B0\": 32}, viz=True):\n",
        "    B = dict(B)\n",
        "    if \"N1\" in nelem and \"B1\" not in B:\n",
        "        B[\"B1\"] = 32\n",
        "    if \"N2\" in nelem and \"B2\" not in B:\n",
        "        B[\"B2\"] = 32\n",
        "\n",
        "    triton_viz.interpreter.record_builder.reset()\n",
        "    torch.manual_seed(0)\n",
        "    signature = inspect.signature(puzzle_spec)\n",
        "    args = {}\n",
        "    for n, p in signature.parameters.items():\n",
        "        print(p)\n",
        "        args[n + \"_ptr\"] = ([d.size for d in p.annotation.dims], p)\n",
        "    args[\"z_ptr\"] = ([d.size for d in signature.return_annotation.dims], None)\n",
        "\n",
        "    tt_args = []\n",
        "    for k, (v, t) in args.items():\n",
        "        tt_args.append(torch.rand(*v))\n",
        "        if t is not None and t.annotation.dtypes[0] == \"int32\":\n",
        "            tt_args[-1] = torch.randint(-100000, 100000, v)\n",
        "    grid = lambda meta: (triton.cdiv(nelem[\"N0\"], meta[\"B0\"]),\n",
        "                         triton.cdiv(nelem.get(\"N1\", 1), meta.get(\"B1\", 1)),\n",
        "                         triton.cdiv(nelem.get(\"N2\", 1), meta.get(\"B2\", 1)))\n",
        "\n",
        "    #for k, v in args.items():\n",
        "    #    print(k, v)\n",
        "    triton_viz.trace(puzzle)[grid](*tt_args, **B, **nelem)\n",
        "    z = tt_args[-1]\n",
        "    tt_args = tt_args[:-1]\n",
        "    z_ = puzzle_spec(*tt_args)\n",
        "    match = torch.allclose(z, z_, rtol=1e-3, atol=1e-3)\n",
        "    print(\"Results match:\",  match)\n",
        "    if viz:\n",
        "        failures = triton_viz.launch()\n",
        "    if not match or failures:\n",
        "        print(\"Invalid Access:\", failures)\n",
        "        print(\"Yours:\", z)\n",
        "        print(\"Spec:\", z_)\n",
        "        print(torch.isclose(z, z_))\n",
        "        return\n",
        "    # PUPPIES!\n",
        "    from IPython.display import HTML\n",
        "    import random\n",
        "    print(\"Correct!\")\n",
        "    pups = [\n",
        "    \"2m78jPG\",\n",
        "    \"pn1e9TO\",\n",
        "    \"MQCIwzT\",\n",
        "    \"udLK6FS\",\n",
        "    \"ZNem5o3\",\n",
        "    \"DS2IZ6K\",\n",
        "    \"aydRUz8\",\n",
        "    \"MVUdQYK\",\n",
        "    \"kLvno0p\",\n",
        "    \"wScLiVz\",\n",
        "    \"Z0TII8i\",\n",
        "    \"F1SChho\",\n",
        "    \"9hRi2jN\",\n",
        "    \"lvzRF3W\",\n",
        "    \"fqHxOGI\",\n",
        "    \"1xeUYme\",\n",
        "    \"6tVqKyM\",\n",
        "    \"CCxZ6Wr\",\n",
        "    \"lMW0OPQ\",\n",
        "    \"wHVpHVG\",\n",
        "    \"Wj2PGRl\",\n",
        "    \"HlaTE8H\",\n",
        "    \"k5jALH0\",\n",
        "    \"3V37Hqr\",\n",
        "    \"Eq2uMTA\",\n",
        "    \"Vy9JShx\",\n",
        "    \"g9I2ZmK\",\n",
        "    \"Nu4RH7f\",\n",
        "    \"sWp0Dqd\",\n",
        "    \"bRKfspn\",\n",
        "    \"qawCMl5\",\n",
        "    \"2F6j2B4\",\n",
        "    \"fiJxCVA\",\n",
        "    \"pCAIlxD\",\n",
        "    \"zJx2skh\",\n",
        "    \"2Gdl1u7\",\n",
        "    \"aJJAY4c\",\n",
        "    \"ros6RLC\",\n",
        "    \"DKLBJh7\",\n",
        "    \"eyxH0Wc\",\n",
        "    \"rJEkEw4\"]\n",
        "    return HTML(\"\"\"\n",
        "    <video alt=\"test\" controls autoplay=1>\n",
        "        <source src=\"https://openpuppies.com/mp4/%s.mp4\"  type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\"%(random.sample(pups, 1)[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBUPv4RIB2bp"
      },
      "source": [
        "## Puzzle 1: Constant Add\n",
        "\n",
        "Add a constant to a vector. Uses one program id axis. Block size `B0` is always the same as vector `x` with length `N0`.\n",
        "\n",
        "\n",
        "$$z_i = 10 + x_i \\text{ for } i = 1\\ldots N_0$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAW5jPSUA7mf"
      },
      "outputs": [],
      "source": [
        "def add_spec(x: Float32[Tensor, \"32\"]) -> Float32[Tensor, \"32\"]:\n",
        "    \"This is the spec that you should implement. Uses typing to define sizes.\"\n",
        "    return x + 10.\n",
        "\n",
        "@triton.jit\n",
        "def add_kernel(x_ptr, z_ptr, N0, B0: tl.constexpr):\n",
        "    range = tl.arange(0, B0)\n",
        "    x = tl.load(x_ptr + range)\n",
        "    z = x + 10\n",
        "    tl.store(z_ptr + range, z)\n",
        "\n",
        "test(add_kernel, add_spec, nelem={\"N0\": 32}, viz=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6j1fdwOCoIQ"
      },
      "source": [
        "## Puzzle 2: Constant Add Block\n",
        "\n",
        "Add a constant to a vector. Uses one program block. Block size `B0` is now smaller than the shape vector `x` which is `N0`.\n",
        "\n",
        "\n",
        "$$z_i = 10 + x_i \\text{ for } i = 1\\ldots N_0$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W944E3gjCbiR"
      },
      "outputs": [],
      "source": [
        "def add2_spec(x: Float32[Tensor, \"200\"]) -> Float32[Tensor, \"200\"]:\n",
        "    return x + 10.\n",
        "\n",
        "@triton.jit\n",
        "def add_mask2_kernel(x_ptr, z_ptr, N0, B0: tl.constexpr):\n",
        "    pid = tl.program_id(axis=0)\n",
        "    offsets = pid*B0 + tl.arange(0, B0)\n",
        "    mask = offsets < N0\n",
        "    x = tl.load(x_ptr + offsets, mask=mask)\n",
        "    z = x + 10\n",
        "    tl.store(z_ptr + offsets, z, mask=mask)\n",
        "\n",
        "test(add_mask2_kernel, add2_spec, nelem={\"N0\": 200})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYPReEU6D3Sn"
      },
      "source": [
        "## Puzzle 3: Outer Vector Add\n",
        "\n",
        "Add two vectors.\n",
        "\n",
        "Uses one program block axis. Block size `B0` is always the same as vector `x` length `N0`.\n",
        "Block size `B1` is always the same as vector `y` length `N1`.\n",
        "\n",
        "\n",
        "$$z_{i, j} = x_i + y_j\\text{ for } i = 1\\ldots B_0,\\ j = 1\\ldots B_1$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sMtmFYUEDip5"
      },
      "outputs": [],
      "source": [
        "def add_vec_spec(x: Float32[Tensor, \"32\"], y: Float32[Tensor, \"32\"]) -> Float32[Tensor, \"32 32\"]:\n",
        "    return x[None, :] + y[:, None]\n",
        "\n",
        "@triton.jit\n",
        "def add_vec_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr):\n",
        "    x_offsets = tl.arange(0, B0)\n",
        "    y_offsets = tl.arange(0, B1)\n",
        "    z_offsets = x_offsets[None, :] + y_offsets[:, None]*B0\n",
        "    x = tl.load(x_ptr + x_offsets)\n",
        "    y = tl.load(y_ptr + y_offsets)\n",
        "    z = x[None, :] + y[:, None]\n",
        "    tl.store(z_ptr + z_offsets, z)\n",
        "\n",
        "test(add_vec_kernel, add_vec_spec, nelem={\"N0\": 32, \"N1\": 32})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNv2rQUkJTQg"
      },
      "source": [
        "## Puzzle 4: Outer Vector Add Block\n",
        "\n",
        "Add a row vector to a column vector.\n",
        "\n",
        "Uses two program block axes. Block size `B0` is always less than the vector `x` length `N0`.\n",
        "Block size `B1` is always less than vector `y` length `N1`.\n",
        "\n",
        "$$z_{i, j} = x_i + y_j\\text{ for } i = 1\\ldots N_0,\\ j = 1\\ldots N_1$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lNP-9n9nIFdk"
      },
      "outputs": [],
      "source": [
        "def add_vec_block_spec(x: Float32[Tensor, \"100\"], y: Float32[Tensor, \"90\"]) -> Float32[Tensor, \"90 100\"]:\n",
        "    return x[None, :] + y[:, None]\n",
        "\n",
        "@triton.jit\n",
        "def add_vec_block_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    pid_1 = tl.program_id(1)\n",
        "    x_offsets = (pid_0*B0 + tl.arange(0, B0))\n",
        "    y_offsets = (pid_1*B1 + tl.arange(0, B1))\n",
        "    z_offsets = x_offsets[None, :] + y_offsets[:, None] * N0\n",
        "\n",
        "    x = tl.load(x_ptr + x_offsets, mask=(x_offsets < N0))\n",
        "    y = tl.load(y_ptr + y_offsets, mask=(y_offsets < N1))\n",
        "    tl.store(z_ptr + z_offsets, x[None, :] + y[:, None], mask = (x_offsets[None, :] < N0) & (y_offsets[:, None] < N1))\n",
        "\n",
        "test(add_vec_block_kernel, add_vec_block_spec, nelem={\"N0\": 100, \"N1\": 90})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LDMlAj1WJOQ"
      },
      "source": [
        "## Puzzle 5: Fused Outer Multiplication\n",
        "\n",
        "Multiply a row vector to a column vector and take a relu.\n",
        "\n",
        "Uses two program block axes. Block size `B0` is always less than the vector `x` length `N0`.\n",
        "Block size `B1` is always less than vector `y` length `N1`.\n",
        "\n",
        "$$z_{i, j} = \\text{relu}(x_i \\times y_j)\\text{ for } i = 1\\ldots N_0,\\ j = 1\\ldots N_1$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_d9Z1mVwWKAP"
      },
      "outputs": [],
      "source": [
        "def mul_relu_block_spec(x: Float32[Tensor, \"100\"], y: Float32[Tensor, \"90\"]) -> Float32[Tensor, \"90 100\"]:\n",
        "    return torch.relu(x[None, :] * y[:, None])\n",
        "\n",
        "@triton.jit\n",
        "def mul_relu_block_kernel(x_ptr, y_ptr, z_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    pid_1 = tl.program_id(1)\n",
        "    x_off = (pid_0*B0 + tl.arange(0, B0))\n",
        "    y_off = (pid_1*B1 + tl.arange(0, B1))\n",
        "    z_off = x_off[None, :] + y_off[:, None] * N0\n",
        "\n",
        "    x = tl.load(x_ptr + x_off, mask=(x_off < N0))\n",
        "    y = tl.load(y_ptr + y_off, mask=(y_off < N1))\n",
        "    z = x[None, :] * y[:, None]\n",
        "    z = tl.where(z>0, z, 0)\n",
        "    tl.store(z_ptr + z_off, z, mask=(x_off[None, :] < N0)&(y_off[:, None]<N1))\n",
        "    return\n",
        "\n",
        "test(mul_relu_block_kernel, mul_relu_block_spec, nelem={\"N0\": 100, \"N1\": 90})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XScGqXR6WUd9"
      },
      "source": [
        "## Puzzle 6: Fused Outer Multiplication - Backwards\n",
        "\n",
        "\n",
        "Backwards of a function that multiplies a matrix with a row vector and take a relu.\n",
        "\n",
        "Uses two program blocks. Block size `B0` is always less than the vector `x` length `N0`.\n",
        "Block size `B1` is always less than vector `y` length `N1`. Chain rule backward `dz`\n",
        "is of shape `N0`\n",
        "\n",
        "$$f(x, y) = \\text{relu}(x_i \\times y_j)\\text{ for } i = 1\\ldots N_0,\\ j = 1\\ldots N_1$$\n",
        "\n",
        "$$dx_{i, j} = f_x'(x, y)_{i, j} \\times dz_{i,j}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lVLBvWxhWVEb"
      },
      "outputs": [],
      "source": [
        "def mul_relu_block_back_spec(x: Float32[Tensor, \"90 100\"], y: Float32[Tensor, \"90\"],\n",
        "                             dz: Float32[Tensor, \"90 100\"]) -> Float32[Tensor, \"90 100\"]:\n",
        "    x = x.clone()\n",
        "    y = y.clone()\n",
        "    x = x.requires_grad_(True)\n",
        "    y = y.requires_grad_(True)\n",
        "    z = torch.relu(x * y[:, None])\n",
        "    z.backward(dz)\n",
        "    dx = x.grad\n",
        "    return dx\n",
        "\n",
        "@triton.jit\n",
        "def mul_relu_block_back_kernel(x_ptr, y_ptr, dz_ptr, dx_ptr, N0, N1, B0: tl.constexpr, B1: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    pid_1 = tl.program_id(1)\n",
        "    x_off_1 = (pid_0*B0 + tl.arange(0, B0))[None, :]\n",
        "    x_off_2 = (pid_1*B1 + tl.arange(0, B1))[:, None]\n",
        "\n",
        "    y_off = pid_1*B1 + tl.arange(0, B1)\n",
        "    dz_off_1 = (pid_0*B0 + tl.arange(0, B0))[None, :]\n",
        "    dz_off_2 = (pid_1*B1 + tl.arange(0, B1))[:, None]\n",
        "\n",
        "    x = tl.load(x_ptr + x_off_1 + x_off_2*N0, mask=(x_off_1 < N0)&(x_off_2 < N1))\n",
        "    y = tl.load(y_ptr + y_off, mask=(y_off < N1))\n",
        "    dz = tl.load(dz_ptr + dz_off_1 + dz_off_2*N0, mask=(dz_off_1 < N0)&(dz_off_2 < N1))\n",
        "\n",
        "    relu_prime = tl.where(x*y[:, None] < 0, 0, 1) * y[:, None]\n",
        "    dx = relu_prime * dz\n",
        "    tl.store(dx_ptr + x_off_1 + x_off_2*N0, dx, mask=(x_off_1 < N0)&(x_off_2 < N1))\n",
        "    return\n",
        "\n",
        "test(mul_relu_block_back_kernel, mul_relu_block_back_spec, nelem={\"N0\": 100, \"N1\": 90})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8yLGx3wWaWa"
      },
      "source": [
        "## Puzzle 7: Long Sum\n",
        "\n",
        "Sum of a batch of numbers.\n",
        "\n",
        "Uses one program blocks. Block size `B0` represents a range of batches of  `x` of length `N0`.\n",
        "Each element is of length `T`. Process it `B1 < T` elements at a time.  \n",
        "\n",
        "$$z_{i} = \\sum^{T}_j x_{i,j} =  \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "Hint: You will need a for loop for this problem. These work and look the same as in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iYSaFTGpWay6"
      },
      "outputs": [],
      "source": [
        "def sum_spec(x: Float32[Tensor, \"4 200\"]) -> Float32[Tensor, \"4\"]:\n",
        "    return x.sum(1)\n",
        "\n",
        "@triton.jit\n",
        "def sum_kernel(x_ptr, z_ptr, N0, N1, T, B0: tl.constexpr, B1: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    sum = tl.full((B0,), 0.0, dtype=tl.float32)\n",
        "\n",
        "    x_off = (pid_0*B0 + tl.arange(0, B0))[:, None]\n",
        "\n",
        "    offsets = tl.arange(0, B1)[None, :]\n",
        "    for i in range(0, T, B1):\n",
        "      x = tl.load(x_ptr + x_off*T +offsets, mask=(x_off<N0)&(offsets<T), other=0.0)\n",
        "      sum += tl.sum(x, axis=1, keep_dims=True)\n",
        "      offsets += B1\n",
        "\n",
        "    tl.store(z_ptr + x_off, sum, mask=(x_off<N0))\n",
        "    return\n",
        "\n",
        "test(sum_kernel, sum_spec, B={\"B0\": 1, \"B1\": 32}, nelem={\"N0\": 4, \"N1\": 32, \"T\": 200})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ehz4QNtWejt"
      },
      "source": [
        "## Puzzle 8: Long Softmax\n",
        "\n",
        "\n",
        "Softmax of a batch of logits.\n",
        "\n",
        "Uses one program block axis. Block size `B0` represents the batch of `x` of length `N0`.\n",
        "Block logit length `T`.   Process it `B1 < T` elements at a time.  \n",
        "\n",
        "$$z_{i, j} = \\text{softmax}(x_{i,1} \\ldots x_{i, T}) \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "Note softmax needs to be computed in numerically stable form as in Python. In addition in Triton they recommend not using `exp` but instead using `exp2`. You need the identity\n",
        "\n",
        "$$\\exp(x) = 2^{\\log_2(e) x}$$\n",
        "\n",
        "Advanced: there one way to do this with 3 loops. You can also do it with 2 loops if you are clever. Hint: you will find this identity useful:\n",
        "\n",
        "$$\\exp(x_i - m) =  \\exp(x_i - m/2 - m/2) = \\exp(x_i - m/ 2) /  \\exp(m/2) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "04LylOleWc82"
      },
      "outputs": [],
      "source": [
        "def softmax_spec(x: Float32[Tensor, \"4 200\"]) -> Float32[Tensor, \"4 200\"]:\n",
        "    x_max = x.max(1, keepdim=True)[0]\n",
        "    x = x - x_max\n",
        "    x_exp = x.exp()\n",
        "    return x_exp / x_exp.sum(1, keepdim=True)\n",
        "\n",
        "@triton.jit\n",
        "def softmax_kernel(x_ptr, z_ptr, N0, N1, T, B0: tl.constexpr, B1: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    log2_e = 1.44269504\n",
        "\n",
        "    _max = tl.full((B0,), -float('inf'), dtype=tl.float32)\n",
        "    denom = tl.full((B0,), 0.0, dtype=tl.float32)\n",
        "\n",
        "    x_off = (pid_0*B0 + tl.arange(0, B0))[:, None]\n",
        "    x_ptrs = x_ptr + x_off*T\n",
        "    # loop once through blocks and update the denom as it goes along\n",
        "    offsets = tl.arange(0, B1)[None, :]\n",
        "    for i in range(0, T, B1):\n",
        "      x = tl.load(x_ptrs + offsets, mask=(x_off < N0) & (offsets<T), other=-float('inf'))\n",
        "      m = tl.max(x, axis=1, keep_dims=True)\n",
        "      new_max = tl.where(m > _max, m, _max)\n",
        "      x = tl.exp2(log2_e * (x - new_max))\n",
        "      denom = tl.exp2(log2_e * (_max - new_max)) * denom + tl.sum(x, axis=1, keep_dims=True)\n",
        "      _max = new_max\n",
        "\n",
        "      offsets += B1\n",
        "\n",
        "    offsets = tl.arange(0, B1)[None, :]\n",
        "    for i in range(0, T, B1):\n",
        "      x = tl.load(x_ptrs + offsets, mask=(x_off < N0) & (offsets<T), other=-float('inf'))\n",
        "      x = tl.exp2(log2_e*(x - _max))\n",
        "      z = x / denom\n",
        "      tl.store(z_ptr + x_off*T + offsets, z, mask=(x_off < N0) & (offsets<T))\n",
        "      offsets += B1\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "test(softmax_kernel, softmax_spec, B={\"B0\": 1, \"B1\":32}, nelem={\"N0\": 4, \"N1\": 32, \"T\": 200})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KA_p79YWi_j"
      },
      "source": [
        "## Puzzle 9: Simple FlashAttention\n",
        "\n",
        "A scalar version of FlashAttention.\n",
        "\n",
        "Uses zero programs. Block size `B0` represents `k` of length `N0`.\n",
        "Block size `B0` represents `q` of length `N0`. Block size `B0` represents `v` of length `N0`.\n",
        "Sequence length is `T`. Process it `B1 < T` elements at a time.  \n",
        "\n",
        "$$z_{i} = \\sum_{j} \\text{softmax}(q_1 k_1, \\ldots, q_T k_T)_j v_{j} \\text{ for } i = 1\\ldots N_0$$\n",
        "\n",
        "This can be done in 1 loop using a similar trick from the last puzzle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0czD9d50WlYo"
      },
      "outputs": [],
      "source": [
        "def flashatt_spec(q: Float32[Tensor, \"200\"], k: Float32[Tensor, \"200\"], v: Float32[Tensor, \"200\"]) -> Float32[Tensor, \"200\"]:\n",
        "    x = q[:, None] * k[None, :]\n",
        "    x_max = x.max(1, keepdim=True)[0]\n",
        "    x = x - x_max\n",
        "    x_exp = x.exp()\n",
        "    soft =  x_exp  / x_exp.sum(1, keepdim=True)\n",
        "    return (v[None, :] * soft).sum(1)\n",
        "\n",
        "@triton.jit\n",
        "def flashatt_kernel(q_ptr, k_ptr, v_ptr, z_ptr, N0, T, B0: tl.constexpr):\n",
        "    log2_e = 1.44269504\n",
        "    B1 = 32\n",
        "\n",
        "    num_blocks = (T // B1) + 1\n",
        "    offsets = tl.arange(0, B1)\n",
        "\n",
        "    for i in range(0, num_blocks):\n",
        "      q = tl.load(q_ptr + offsets + i*B1, mask=offsets+i*B1 < N0, other=-float('inf'))\n",
        "\n",
        "      maxes = tl.full((B1,), -float('inf'), tl.float32)\n",
        "      denoms = tl.full((B1,), 0.0, dtype=tl.float32)\n",
        "      z = tl.full((B1,), 0.0, dtype=tl.float32)\n",
        "\n",
        "      for j in range(0, num_blocks):\n",
        "        k = tl.load(k_ptr + offsets + j*B1, mask=offsets+j*B1 < N0, other=-float('inf'))\n",
        "        v = tl.load(v_ptr + offsets + j*B1, mask=offsets+j*B1 < N0, other=0.0)\n",
        "\n",
        "        s = q[:, None] * k[None, :]\n",
        "        row_maxes = tl.max(s, axis=1)\n",
        "        p = tl.exp2(log2_e * (s - tl.max(s, axis=1, keep_dims=True)))\n",
        "        new_maxes = tl.where(row_maxes > maxes, row_maxes, maxes)\n",
        "\n",
        "        new_denoms = denoms*tl.exp2(log2_e*(maxes-new_maxes)) + tl.sum(p, axis=1)*tl.exp2(log2_e*(row_maxes-new_maxes))\n",
        "\n",
        "        new_z = p * v[None, :]\n",
        "        z = (denoms * z * tl.exp2(log2_e*(maxes-new_maxes)) + tl.sum(new_z, axis=1)*tl.exp2(log2_e*(row_maxes-new_maxes))) / new_denoms\n",
        "\n",
        "        maxes = new_maxes\n",
        "        denoms = new_denoms\n",
        "\n",
        "      tl.store(z_ptr + offsets + i*B1, z, mask=offsets+i*B1<N0)\n",
        "\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "test(flashatt_kernel, flashatt_spec, B={\"B0\":200},\n",
        "     nelem={\"N0\": 200, \"T\": 200})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVbcKw2NWoej"
      },
      "source": [
        "## Puzzle 10: Two Dimensional Convolution\n",
        "\n",
        "A batched 2D convolution.\n",
        "\n",
        "Uses one program id axis. Block size `B0` represent the batches to process out of `N0`.\n",
        "Image `x` is size is `H` by `W` with only 1 channel, and kernel `k` is size `KH` by `KW`.\n",
        "\n",
        "$$z_{i, j, k} = \\sum_{oj, ok} k_{oj,ok} \\times x_{i,j + oj, k + ok} \\text{ for } i = 1\\ldots N_0$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C89SF8COWqpZ"
      },
      "outputs": [],
      "source": [
        "def conv2d_spec(x: Float32[Tensor, \"4 8 8\"], k: Float32[Tensor, \"4 4\"]) -> Float32[Tensor, \"4 8 8\"]:\n",
        "    z = torch.zeros(4, 8, 8)\n",
        "    x = torch.nn.functional.pad(x, (0, 4, 0, 4, 0, 0), value=0.0)\n",
        "    print(x.shape, k.shape)\n",
        "    for i in range(8):\n",
        "        for j in range(8):\n",
        "            z[:, i, j] = (k[None, :, :] * x[:, i: i+4, j: j + 4]).sum(1).sum(1)\n",
        "    return z\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def conv2d_kernel(x_ptr, k_ptr, z_ptr, N0, H, W, KH: tl.constexpr, KW: tl.constexpr, B0: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    k_ptrs = k_ptr + tl.arange(0, KW)[None, :] + KW*tl.arange(0, KH)[:, None]\n",
        "    k = tl.load(k_ptrs)\n",
        "\n",
        "    x_base = (pid_0*B0 + tl.arange(0, B0))[:, None, None]\n",
        "    x_base = x_base*H*W\n",
        "\n",
        "    for idx in range(H*W):\n",
        "      i = idx % W\n",
        "      j = idx //  W\n",
        "      x_off = (i+tl.arange(0, KW))[None, None, :] + W*(j+tl.arange(0, KH))[None, :, None]\n",
        "      mask = ((i+tl.arange(0, KW))[None, None, :] < W) & ((j+tl.arange(0, KH))[None, :, None] < H)\n",
        "      x = tl.load(x_ptr+x_base+x_off, mask=mask, other=0.0)\n",
        "      mult = (k[None, :, :] * x[:, :, :])\n",
        "      z = tl.sum(tl.sum(mult, axis=2, keep_dims=True), axis=1, keep_dims=True)\n",
        "\n",
        "      z_off = (i+tl.arange(0, 1))[None, None, :] + W*(j+tl.arange(0, 1))[None, :, None]\n",
        "\n",
        "\n",
        "      tl.store(z_ptr+x_base+z_off, z)\n",
        "\n",
        "    return\n",
        "\n",
        "test(conv2d_kernel, conv2d_spec, B={\"B0\": 1}, nelem={\"N0\": 4, \"H\": 8, \"W\": 8, \"KH\": 4, \"KW\": 4})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibHuycexWsvW"
      },
      "source": [
        "## Puzzle 11: Matrix Multiplication\n",
        "\n",
        "A blocked matrix multiplication.\n",
        "\n",
        "Uses three program id axes. Block size `B2` represent the batches to process out of `N2`.\n",
        "Block size `B0` represent the rows of `x` to process out of `N0`. Block size `B1` represent the cols of `y` to process out of `N1`. The middle shape is `MID`.\n",
        "\n",
        "$$z_{i, j, k} = \\sum_{k} x_{i,j, l} \\times y_{i, l, k} \\text{ for } i = 1\\ldots N_2, j = 1\\ldots N_0, k = 1\\ldots N_1$$\n",
        "\n",
        "You are allowed to use `tl.dot` which computes a smaller mat mul.\n",
        "\n",
        "Hint: the main trick is that you can split a matmul into smaller parts.\n",
        "\n",
        "$$z_{i, j, k} = \\sum_{k=1}^{K/2} x_{i,j, l} \\times y_{i, l, k} +  \\sum_{k=K/2}^{K} x_{i,j, l} \\times y_{i, l, k} $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvXfJvZBWur0"
      },
      "outputs": [],
      "source": [
        "def dot_spec(x: Float32[Tensor, \"4 32 32\"], y: Float32[Tensor, \"4 32 32\"]) -> Float32[Tensor, \"4 32 32\"]:\n",
        "    return x @ y\n",
        "\n",
        "@triton.jit\n",
        "def dot_kernel(x_ptr, y_ptr, z_ptr, N0, N1, N2, MID, B0: tl.constexpr, B1: tl.constexpr, B2: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    pid_1 = tl.program_id(1)\n",
        "    pid_2 = tl.program_id(2)\n",
        "\n",
        "    acc = tl.full((B2, B0, B1), 0.0, tl.float32)\n",
        "\n",
        "    batch_dims = (pid_2*B2 + tl.arange(0, B2))[:, None, None]\n",
        "    y_row_dims = (tl.arange(0, B0))[None, :, None]\n",
        "    y_col_dims = (pid_1*B1 + tl.arange(0, B1))[None, None, :]\n",
        "    x_row_dims = (pid_0*B0 + tl.arange(0, B0))[None, :, None]\n",
        "    x_col_dims = (tl.arange(0, B1))[None, None, :]\n",
        "\n",
        "    x_base = x_row_dims*MID + x_col_dims\n",
        "    y_base = y_row_dims*N1 + y_col_dims\n",
        "    z_base = batch_dims*N1*N0 + x_row_dims*N1 + y_col_dims\n",
        "\n",
        "    off_mid = tl.arange(0, B1)\n",
        "\n",
        "\n",
        "    num = MID // B1\n",
        "    for k in range(num):\n",
        "      x = tl.load(x_ptr + batch_dims*N1*N0 + x_base, mask=off_mid[None, None, :] < (B1 + k*B1), other=0.0)\n",
        "      y = tl.load(y_ptr + batch_dims*N1*N0 + y_base, mask=off_mid[None, :, None] < (B1 + k*B1), other=0.0)\n",
        "\n",
        "      acc += tl.dot(x, y)\n",
        "      x_base += B1\n",
        "      y_base += B1*MID\n",
        "\n",
        "    z_mask = (x_row_dims < N0) & (y_col_dims < N1) & (batch_dims < N2)\n",
        "    tl.store(z_ptr + z_base, acc, mask=z_mask)\n",
        "    return\n",
        "\n",
        "\n",
        "test(dot_kernel, dot_spec, B={\"B0\": 16, \"B1\": 16, \"B2\": 1}, nelem={\"N0\": 32, \"N1\": 32, \"N2\": 4, \"MID\": 32})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_6VCSJlWw0N"
      },
      "source": [
        "## Puzzle 12: Quantized Matrix Mult\n",
        "\n",
        "When doing matrix multiplication with quantized neural networks a common strategy is to store the weight matrix in lower precision, with a shift and scale term.\n",
        "\n",
        "For this problem our `weight` will be stored in 4 bits. We can store `FPINT` of these in a 32 bit integer. In addition for every `group` weights in order we will store 1 `scale` float value and 1 `shift` 4 bit value. We store these for the column of weight. The `activation`s are stored separately in standard floats.\n",
        "\n",
        "Mathematically it looks like.\n",
        "\n",
        "$$z_{j, k} = \\sum_{k} sc_{j, l/g} (w_{j, l} - sh_{j, l/g}) \\times y_{l, k} \\text{ for } i = 1\\ldots N_2, j = 1\\ldots N_0, k = 1\\ldots N_1$$\n",
        "\n",
        "However, it is a bit more complex since we need to also extract the 4-bit values into floats to begin.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUIgaiBmW2eW"
      },
      "outputs": [],
      "source": [
        "\n",
        "FPINT = 32 // 4\n",
        "GROUP = 8\n",
        "\n",
        "def quant_dot_spec(scale : Float32[Tensor, \"32 8\"],\n",
        "                   offset : Int32[Tensor, \"32\"],\n",
        "                   weight: Int32[Tensor, \"32 8\"],\n",
        "                   activation: Float32[Tensor, \"64 32\"]) -> Float32[Tensor, \"32 32\"]:\n",
        "    offset = offset.view(32, 1)\n",
        "    def extract(x):\n",
        "        over = torch.arange(8) * 4\n",
        "        mask = 2**4 - 1\n",
        "        return (x[..., None] >> over) & mask\n",
        "    scale = scale[..., None].expand(-1, 8, GROUP).contiguous().view(-1, 64)\n",
        "    offset = extract(offset)[..., None].expand(-1, 1, 8, GROUP).contiguous().view(-1, 64)\n",
        "    return ( scale * (extract(weight).view(-1, 64) - offset))  @ activation\n",
        "\n",
        "@triton.jit\n",
        "def quant_dot_kernel(scale_ptr, offset_ptr, weight_ptr, activation_ptr,\n",
        "                     z_ptr, N0, N1, MID, B0: tl.constexpr, B1: tl.constexpr):\n",
        "    pid_0 = tl.program_id(0)\n",
        "    pid_1 = tl.program_id(1)\n",
        "    '''\n",
        "    create X by loading scale, offset, weight and constructing\n",
        "    the correct weight matrix\n",
        "    then do the normal block level matrix mult\n",
        "    '''\n",
        "\n",
        "    acc = tl.full((B0, B1), 0.0, tl.float32)\n",
        "\n",
        "    K = B1 // FPINT\n",
        "    X_STRIDE = MID // FPINT\n",
        "\n",
        "    y_row_dims = (tl.arange(0, B0))[:, None]\n",
        "    y_col_dims = (pid_1*B1 + tl.arange(0, B1))[None, :]\n",
        "    x_row_dims = (pid_0*B0 + tl.arange(0, B0))[:, None]\n",
        "    x_col_dims = (tl.arange(0, K))[None, :]\n",
        "\n",
        "    x_base = x_row_dims*X_STRIDE + x_col_dims\n",
        "    y_base = y_row_dims*N1 + y_col_dims\n",
        "    z_base = x_row_dims*N1 + y_col_dims\n",
        "\n",
        "    # need to figure out how this _actually_ works\n",
        "    over = tl.arange(0, FPINT)*4\n",
        "    mask = 2**4 - 1\n",
        "\n",
        "    offset_dims = x_row_dims\n",
        "    offset = tl.load(offset_ptr + offset_dims).reshape(B0, 1)\n",
        "    offset = ((offset[:, :, None] >> over) & mask).to(tl.float32).reshape(B0, FPINT)\n",
        "\n",
        "\n",
        "    num = MID // B1\n",
        "    for i in range(num):\n",
        "\n",
        "      scale = tl.load(scale_ptr + x_base)\n",
        "\n",
        "      x = tl.load(weight_ptr + x_base)\n",
        "      y = tl.load(activation_ptr + y_base)\n",
        "      weight = ((x[:, :, None] >> over) & mask).to(tl.float32)\n",
        "      weight = weight.reshape(B0, B1)\n",
        "\n",
        "\n",
        "      #weight = (weight-offset)\n",
        "      #weight = scale*weight\n",
        "      acc += tl.dot(weight, y)\n",
        "\n",
        "      x_base += K\n",
        "      y_base += N1*B1\n",
        "\n",
        "    tl.store(z_ptr + z_base, acc)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test(quant_dot_kernel, quant_dot_spec, B={\"B0\": 16, \"B1\": 16},\n",
        "                                       nelem={\"N0\": 32, \"N1\": 32, \"MID\": 64})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "over = torch.arange(8)*4\n",
        "mask = 2**4 - 1"
      ],
      "metadata": {
        "id": "nses-U0e6hNe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "offset = torch.tensor([0, 1, 2], dtype=torch.int32).reshape(3, 1)"
      ],
      "metadata": {
        "id": "H8M7_Ojje4gF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = (offset[:, :, None] >> over) & mask"
      ],
      "metadata": {
        "id": "C_vB1oAIPPgz"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = b[:, : :, None]\n",
        "b = b.expand(-1, 1, 8, 8)\n",
        "b.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y6ZO_J514_9",
        "outputId": "1aedc74f-836c-448f-f4c8-5d8463dfebe2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = b.reshape(3, 64)"
      ],
      "metadata": {
        "id": "9vegVwKb20DD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbF2zxHv27sU",
        "outputId": "2abd6d29-9a3c-446d-827d-5f7558bc9369"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
              "         2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
              "         2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.arange(3*8).reshape(3, 8)\n",
        "w = (w[:, :, None] >> over) & mask\n",
        "w = w.reshape(3, 64)"
      ],
      "metadata": {
        "id": "pVcm-HZ-yyCB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA9Vl_OF1qVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}